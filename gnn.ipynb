{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import dgl.nn as dglnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "\n",
    "# Define a Heterograph Conv model\n",
    "\n",
    "class RGCN(nn.Module):\n",
    "    def __init__(self, emb_types, emb_size, hid_feats, out_feats, rel_names):\n",
    "        super().__init__()\n",
    "        # https://www.jianshu.com/p/767950b560c4\n",
    "        embed_dict = {ntype : nn.Parameter(torch.Tensor(emb_types[ntype], emb_size))\n",
    "                      for ntype in emb_types.keys()}\n",
    "        for key, embed in embed_dict.items():\n",
    "            nn.init.xavier_uniform_(embed)\n",
    "        self.embed = nn.ParameterDict(embed_dict)\n",
    "        self.conv1 = dglnn.HeteroGraphConv({\n",
    "            rel: dglnn.GraphConv(emb_size, hid_feats)\n",
    "            for rel in rel_names}, aggregate='sum')\n",
    "        self.conv2 = dglnn.HeteroGraphConv({\n",
    "            rel: dglnn.GraphConv(hid_feats, out_feats)\n",
    "            for rel in rel_names}, aggregate='sum')\n",
    "\n",
    "    def forward(self, graph):\n",
    "        # inputs are features of nodes\n",
    "        h = self.conv1(graph, self.embed)\n",
    "        h = {k: F.relu(v) for k, v in h.items()}\n",
    "        h = self.conv2(graph, h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes={'Brand': 1900, 'Customer': 11041, 'Product': 5334, 'Review': 77071},\n",
       "      num_edges={('Brand', 'rev_SOLD_BY', 'Product'): 5555, ('Customer', 'WROTE', 'Review'): 77071, ('Product', 'SOLD_BY', 'Brand'): 5555, ('Product', 'rev_REVIEW_OF', 'Review'): 77071, ('Review', 'REVIEW_OF', 'Product'): 77071, ('Review', 'rev_WROTE', 'Customer'): 77071},\n",
       "      metagraph=[('Brand', 'Product', 'rev_SOLD_BY'), ('Product', 'Brand', 'SOLD_BY'), ('Product', 'Review', 'rev_REVIEW_OF'), ('Customer', 'Review', 'WROTE'), ('Review', 'Product', 'REVIEW_OF'), ('Review', 'Customer', 'rev_WROTE')])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = dgl.load_graphs(\"./graphs/industrial_and_scientific_5_core.dgl\")[0][0]\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RGCN({x: g.number_of_nodes(x) for x in g.ntypes}, 512, 256, 2, g.etypes)\n",
    "\n",
    "labels = g.nodes[\"Review\"].data[\"Positive\"]\n",
    "train_mask = g.nodes[\"Review\"].data[\"train_mask\"]\n",
    "test_mask = g.nodes[\"Review\"].data[\"test_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 0.6928020119667053\n",
      "Epoch: 1 Loss: 0.671084463596344\n",
      "Epoch: 2 Loss: 0.6490806937217712\n",
      "Epoch: 3 Loss: 0.6235765814781189\n",
      "Epoch: 4 Loss: 0.5933032631874084\n",
      "Epoch: 5 Loss: 0.5576277375221252\n",
      "Epoch: 6 Loss: 0.5167205333709717\n",
      "Epoch: 7 Loss: 0.47172102332115173\n",
      "Epoch: 8 Loss: 0.4247688353061676\n",
      "Epoch: 9 Loss: 0.37867745757102966\n",
      "Epoch: 10 Loss: 0.33642256259918213\n",
      "Epoch: 11 Loss: 0.30055058002471924\n",
      "Epoch: 12 Loss: 0.2725561559200287\n",
      "Epoch: 13 Loss: 0.252458393573761\n",
      "Epoch: 14 Loss: 0.23884126543998718\n",
      "Epoch: 15 Loss: 0.22939857840538025\n",
      "Epoch: 16 Loss: 0.22171899676322937\n",
      "Epoch: 17 Loss: 0.21393801271915436\n",
      "Epoch: 18 Loss: 0.20503370463848114\n",
      "Epoch: 19 Loss: 0.19480569660663605\n",
      "Epoch: 20 Loss: 0.18366236984729767\n",
      "Epoch: 21 Loss: 0.1723494976758957\n",
      "Epoch: 22 Loss: 0.16169394552707672\n",
      "Epoch: 23 Loss: 0.15243180096149445\n",
      "Epoch: 24 Loss: 0.14512765407562256\n",
      "Epoch: 25 Loss: 0.14007924497127533\n",
      "Epoch: 26 Loss: 0.13707202672958374\n",
      "Epoch: 27 Loss: 0.13542404770851135\n",
      "Epoch: 28 Loss: 0.13424722850322723\n",
      "Epoch: 29 Loss: 0.1327475905418396\n",
      "Epoch: 30 Loss: 0.1304870992898941\n",
      "Epoch: 31 Loss: 0.1274549663066864\n",
      "Epoch: 32 Loss: 0.12395669519901276\n",
      "Epoch: 33 Loss: 0.12044023722410202\n",
      "Epoch: 34 Loss: 0.1173236146569252\n",
      "Epoch: 35 Loss: 0.11487609893083572\n",
      "Epoch: 36 Loss: 0.11317171156406403\n",
      "Epoch: 37 Loss: 0.11210201680660248\n",
      "Epoch: 38 Loss: 0.11144448071718216\n",
      "Epoch: 39 Loss: 0.11094189435243607\n",
      "Epoch: 40 Loss: 0.11037462204694748\n",
      "Epoch: 41 Loss: 0.10960285365581512\n",
      "Epoch: 42 Loss: 0.10858328640460968\n",
      "Epoch: 43 Loss: 0.10736124217510223\n",
      "Epoch: 44 Loss: 0.10604602843523026\n",
      "Epoch: 45 Loss: 0.10477552562952042\n",
      "Epoch: 46 Loss: 0.10367873311042786\n",
      "Epoch: 47 Loss: 0.10283724963665009\n",
      "Epoch: 48 Loss: 0.10226137936115265\n",
      "Epoch: 49 Loss: 0.10188862681388855\n",
      "Epoch: 50 Loss: 0.10160808265209198\n",
      "Epoch: 51 Loss: 0.10130327939987183\n",
      "Epoch: 52 Loss: 0.10089688748121262\n",
      "Epoch: 53 Loss: 0.10037427395582199\n",
      "Epoch: 54 Loss: 0.09978022426366806\n",
      "Epoch: 55 Loss: 0.0991913378238678\n",
      "Epoch: 56 Loss: 0.0986793115735054\n",
      "Epoch: 57 Loss: 0.09828334301710129\n",
      "Epoch: 58 Loss: 0.09799890965223312\n",
      "Epoch: 59 Loss: 0.09778714925050735\n",
      "Epoch: 60 Loss: 0.09759525954723358\n",
      "Epoch: 61 Loss: 0.09737969934940338\n",
      "Epoch: 62 Loss: 0.09712109714746475\n",
      "Epoch: 63 Loss: 0.0968274250626564\n",
      "Epoch: 64 Loss: 0.0965261235833168\n",
      "Epoch: 65 Loss: 0.09624925255775452\n",
      "Epoch: 66 Loss: 0.09601765871047974\n",
      "Epoch: 67 Loss: 0.0958336889743805\n",
      "Epoch: 68 Loss: 0.09568288177251816\n",
      "Epoch: 69 Loss: 0.09554256498813629\n",
      "Epoch: 70 Loss: 0.09539277851581573\n",
      "Epoch: 71 Loss: 0.09522556513547897\n",
      "Epoch: 72 Loss: 0.09504611790180206\n",
      "Epoch: 73 Loss: 0.09486858546733856\n",
      "Epoch: 74 Loss: 0.09470793604850769\n",
      "Epoch: 75 Loss: 0.09457128494977951\n",
      "Epoch: 76 Loss: 0.09445565193891525\n",
      "Epoch: 77 Loss: 0.09435149282217026\n",
      "Epoch: 78 Loss: 0.09424930810928345\n",
      "Epoch: 79 Loss: 0.09414389729499817\n",
      "Epoch: 80 Loss: 0.09403594583272934\n",
      "Epoch: 81 Loss: 0.09393056482076645\n",
      "Epoch: 82 Loss: 0.09383328258991241\n",
      "Epoch: 83 Loss: 0.09374634176492691\n",
      "Epoch: 84 Loss: 0.09366734325885773\n",
      "Epoch: 85 Loss: 0.09359128028154373\n",
      "Epoch: 86 Loss: 0.09351426362991333\n",
      "Epoch: 87 Loss: 0.0934358760714531\n",
      "Epoch: 88 Loss: 0.09335881471633911\n",
      "Epoch: 89 Loss: 0.09328652173280716\n",
      "Epoch: 90 Loss: 0.09322111308574677\n",
      "Epoch: 91 Loss: 0.09316202253103256\n",
      "Epoch: 92 Loss: 0.09310697019100189\n",
      "Epoch: 93 Loss: 0.09305303543806076\n",
      "Epoch: 94 Loss: 0.09299834072589874\n",
      "Epoch: 95 Loss: 0.09294227510690689\n",
      "Epoch: 96 Loss: 0.09288567304611206\n",
      "Epoch: 97 Loss: 0.09283055365085602\n",
      "Epoch: 98 Loss: 0.09277817606925964\n",
      "Epoch: 99 Loss: 0.09272880852222443\n",
      "Epoch: 100 Loss: 0.09268221259117126\n",
      "Epoch: 101 Loss: 0.09263797104358673\n",
      "Epoch: 102 Loss: 0.09259594976902008\n",
      "Epoch: 103 Loss: 0.09255640208721161\n",
      "Epoch: 104 Loss: 0.09251925349235535\n",
      "Epoch: 105 Loss: 0.0924839898943901\n",
      "Epoch: 106 Loss: 0.09244965761899948\n",
      "Epoch: 107 Loss: 0.09241553395986557\n",
      "Epoch: 108 Loss: 0.09238128364086151\n",
      "Epoch: 109 Loss: 0.09234727919101715\n",
      "Epoch: 110 Loss: 0.09231406450271606\n",
      "Epoch: 111 Loss: 0.09228208661079407\n",
      "Epoch: 112 Loss: 0.09225130826234818\n",
      "Epoch: 113 Loss: 0.09222140163183212\n",
      "Epoch: 114 Loss: 0.09219208359718323\n",
      "Epoch: 115 Loss: 0.09216327965259552\n",
      "Epoch: 116 Loss: 0.09213535487651825\n",
      "Epoch: 117 Loss: 0.0921085923910141\n",
      "Epoch: 118 Loss: 0.09208283573389053\n",
      "Epoch: 119 Loss: 0.09205751866102219\n",
      "Epoch: 120 Loss: 0.09203210473060608\n",
      "Epoch: 121 Loss: 0.09200617671012878\n",
      "Epoch: 122 Loss: 0.09197953343391418\n",
      "Epoch: 123 Loss: 0.09195221960544586\n",
      "Epoch: 124 Loss: 0.09192421287298203\n",
      "Epoch: 125 Loss: 0.09189599752426147\n",
      "Epoch: 126 Loss: 0.09186810255050659\n",
      "Epoch: 127 Loss: 0.09184133261442184\n",
      "Epoch: 128 Loss: 0.09181617945432663\n",
      "Epoch: 129 Loss: 0.09179211407899857\n",
      "Epoch: 130 Loss: 0.09176774322986603\n",
      "Epoch: 131 Loss: 0.09174176305532455\n",
      "Epoch: 132 Loss: 0.09171447902917862\n",
      "Epoch: 133 Loss: 0.0916864201426506\n",
      "Epoch: 134 Loss: 0.09165789932012558\n",
      "Epoch: 135 Loss: 0.09162916243076324\n",
      "Epoch: 136 Loss: 0.09160109609365463\n",
      "Epoch: 137 Loss: 0.09157299995422363\n",
      "Epoch: 138 Loss: 0.09154348820447922\n",
      "Epoch: 139 Loss: 0.09151305258274078\n",
      "Epoch: 140 Loss: 0.09148385375738144\n",
      "Epoch: 141 Loss: 0.09145667403936386\n",
      "Epoch: 142 Loss: 0.09143073111772537\n",
      "Epoch: 143 Loss: 0.09140428900718689\n",
      "Epoch: 144 Loss: 0.09137703478336334\n",
      "Epoch: 145 Loss: 0.09134955704212189\n",
      "Epoch: 146 Loss: 0.0913228914141655\n",
      "Epoch: 147 Loss: 0.09129796922206879\n",
      "Epoch: 148 Loss: 0.09127458184957504\n",
      "Epoch: 149 Loss: 0.09125152975320816\n",
      "Epoch: 150 Loss: 0.09122750908136368\n",
      "Epoch: 151 Loss: 0.09120231121778488\n",
      "Epoch: 152 Loss: 0.09117624163627625\n",
      "Epoch: 153 Loss: 0.09114950150251389\n",
      "Epoch: 154 Loss: 0.09112231433391571\n",
      "Epoch: 155 Loss: 0.09109523147344589\n",
      "Epoch: 156 Loss: 0.09106811136007309\n",
      "Epoch: 157 Loss: 0.09104011207818985\n",
      "Epoch: 158 Loss: 0.09101051837205887\n",
      "Epoch: 159 Loss: 0.09097897261381149\n",
      "Epoch: 160 Loss: 0.090946264564991\n",
      "Epoch: 161 Loss: 0.09091338515281677\n",
      "Epoch: 162 Loss: 0.09088154882192612\n",
      "Epoch: 163 Loss: 0.09085112810134888\n",
      "Epoch: 164 Loss: 0.09082262963056564\n",
      "Epoch: 165 Loss: 0.09079604595899582\n",
      "Epoch: 166 Loss: 0.09077098220586777\n",
      "Epoch: 167 Loss: 0.09074722230434418\n",
      "Epoch: 168 Loss: 0.09072477370500565\n",
      "Epoch: 169 Loss: 0.09070294350385666\n",
      "Epoch: 170 Loss: 0.0906808003783226\n",
      "Epoch: 171 Loss: 0.09065808355808258\n",
      "Epoch: 172 Loss: 0.09063497930765152\n",
      "Epoch: 173 Loss: 0.09061206132173538\n",
      "Epoch: 174 Loss: 0.09058986604213715\n",
      "Epoch: 175 Loss: 0.09056863933801651\n",
      "Epoch: 176 Loss: 0.09054816514253616\n",
      "Epoch: 177 Loss: 0.09052789956331253\n",
      "Epoch: 178 Loss: 0.090507373213768\n",
      "Epoch: 179 Loss: 0.09048642218112946\n",
      "Epoch: 180 Loss: 0.09046528488397598\n",
      "Epoch: 181 Loss: 0.09044452011585236\n",
      "Epoch: 182 Loss: 0.09042417258024216\n",
      "Epoch: 183 Loss: 0.09040384739637375\n",
      "Epoch: 184 Loss: 0.09038344770669937\n",
      "Epoch: 185 Loss: 0.09036317467689514\n",
      "Epoch: 186 Loss: 0.09034379571676254\n",
      "Epoch: 187 Loss: 0.09032590687274933\n",
      "Epoch: 188 Loss: 0.09030970185995102\n",
      "Epoch: 189 Loss: 0.09029462188482285\n",
      "Epoch: 190 Loss: 0.09027967602014542\n",
      "Epoch: 191 Loss: 0.09026439487934113\n",
      "Epoch: 192 Loss: 0.0902489498257637\n",
      "Epoch: 193 Loss: 0.09023404866456985\n",
      "Epoch: 194 Loss: 0.09022022783756256\n",
      "Epoch: 195 Loss: 0.09020745754241943\n",
      "Epoch: 196 Loss: 0.09019529819488525\n",
      "Epoch: 197 Loss: 0.09018327295780182\n",
      "Epoch: 198 Loss: 0.090171217918396\n",
      "Epoch: 199 Loss: 0.0901593342423439\n",
      "Epoch: 200 Loss: 0.09014789760112762\n",
      "Epoch: 201 Loss: 0.0901370570063591\n",
      "Epoch: 202 Loss: 0.09012668579816818\n",
      "Epoch: 203 Loss: 0.09011659771203995\n",
      "Epoch: 204 Loss: 0.09010656923055649\n",
      "Epoch: 205 Loss: 0.09009654819965363\n",
      "Epoch: 206 Loss: 0.09008654952049255\n",
      "Epoch: 207 Loss: 0.09007665514945984\n",
      "Epoch: 208 Loss: 0.09006700664758682\n",
      "Epoch: 209 Loss: 0.09005758911371231\n",
      "Epoch: 210 Loss: 0.09004838764667511\n",
      "Epoch: 211 Loss: 0.09003935754299164\n",
      "Epoch: 212 Loss: 0.0900304764509201\n",
      "Epoch: 213 Loss: 0.09002172201871872\n",
      "Epoch: 214 Loss: 0.09001310914754868\n",
      "Epoch: 215 Loss: 0.09000463038682938\n",
      "Epoch: 216 Loss: 0.08999627083539963\n",
      "Epoch: 217 Loss: 0.08998803794384003\n",
      "Epoch: 218 Loss: 0.08997994661331177\n",
      "Epoch: 219 Loss: 0.08997199684381485\n",
      "Epoch: 220 Loss: 0.08996415138244629\n",
      "Epoch: 221 Loss: 0.08995642513036728\n",
      "Epoch: 222 Loss: 0.08994883298873901\n",
      "Epoch: 223 Loss: 0.08994143456220627\n",
      "Epoch: 224 Loss: 0.08993420004844666\n",
      "Epoch: 225 Loss: 0.08992709219455719\n",
      "Epoch: 226 Loss: 0.08992011100053787\n",
      "Epoch: 227 Loss: 0.08991324156522751\n",
      "Epoch: 228 Loss: 0.08990651369094849\n",
      "Epoch: 229 Loss: 0.0898999348282814\n",
      "Epoch: 230 Loss: 0.08989346772432327\n",
      "Epoch: 231 Loss: 0.0898870974779129\n",
      "Epoch: 232 Loss: 0.08988082408905029\n",
      "Epoch: 233 Loss: 0.08987466245889664\n",
      "Epoch: 234 Loss: 0.08986860513687134\n",
      "Epoch: 235 Loss: 0.0898626372218132\n",
      "Epoch: 236 Loss: 0.08985678106546402\n",
      "Epoch: 237 Loss: 0.08985104411840439\n",
      "Epoch: 238 Loss: 0.08984540402889252\n",
      "Epoch: 239 Loss: 0.0898398607969284\n",
      "Epoch: 240 Loss: 0.08983445167541504\n",
      "Epoch: 241 Loss: 0.08982910960912704\n",
      "Epoch: 242 Loss: 0.08982387185096741\n",
      "Epoch: 243 Loss: 0.08981876075267792\n",
      "Epoch: 244 Loss: 0.08981373906135559\n",
      "Epoch: 245 Loss: 0.08980882912874222\n",
      "Epoch: 246 Loss: 0.0898040160536766\n",
      "Epoch: 247 Loss: 0.08979932218790054\n",
      "Epoch: 248 Loss: 0.08979472517967224\n",
      "Epoch: 249 Loss: 0.0897902175784111\n"
     ]
    }
   ],
   "source": [
    "opt = torch.optim.Adam(model.parameters())\n",
    "\n",
    "for epoch in range(250):\n",
    "    model.train()\n",
    "\n",
    "    logits = model(g)[\"Review\"]\n",
    "    loss = F.cross_entropy(logits[train_mask == 1], labels[train_mask == 1].type(torch.long))\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    print(\"Epoch:\", epoch, \"Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model(g)[\"Review\"][test_mask == 1].detach().numpy()\n",
    "y_test = labels[test_mask == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.24      0.23      0.23      4750\n",
      "         1.0       0.89      0.90      0.90     33786\n",
      "\n",
      "    accuracy                           0.82     38536\n",
      "   macro avg       0.57      0.56      0.56     38536\n",
      "weighted avg       0.81      0.82      0.81     38536\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, preds.argmax(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8960650569395542"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_test, preds.argmax(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9001361510684899"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "recall_score(y_test, preds.argmax(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8920306221218431"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "precision_score(y_test, preds.argmax(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7f0fb73b74445d6392909c9d469b21cbfa1ec308c4aeb674f6b2b586eed9f638"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('huggingface')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
