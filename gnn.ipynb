{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import dgl.nn as dglnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "\n",
    "# Define a Heterograph Conv model\n",
    "\n",
    "class RGCN(nn.Module):\n",
    "    def __init__(self, emb_types, emb_size, hid_feats, out_feats, rel_names):\n",
    "        super().__init__()\n",
    "        # https://www.jianshu.com/p/767950b560c4\n",
    "        embed_dict = {ntype : nn.Parameter(torch.Tensor(emb_types[ntype], emb_size))\n",
    "                      for ntype in emb_types.keys()}\n",
    "        for key, embed in embed_dict.items():\n",
    "            nn.init.xavier_uniform_(embed)\n",
    "        self.embed = nn.ParameterDict(embed_dict)\n",
    "        self.conv1 = dglnn.HeteroGraphConv({\n",
    "            rel: dglnn.GraphConv(emb_size, hid_feats)\n",
    "            for rel in rel_names}, aggregate='sum')\n",
    "        self.conv2 = dglnn.HeteroGraphConv({\n",
    "            rel: dglnn.GraphConv(hid_feats, out_feats)\n",
    "            for rel in rel_names}, aggregate='sum')\n",
    "\n",
    "    def forward(self, graph):\n",
    "        # inputs are features of nodes\n",
    "        h = self.conv1(graph, self.embed)\n",
    "        h = {k: F.relu(v) for k, v in h.items()}\n",
    "        h = self.conv2(graph, h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes={'Brand': 1900, 'Customer': 11041, 'Product': 5334, 'Review': 77071},\n",
       "      num_edges={('Brand', 'rev_SOLD_BY', 'Product'): 5555, ('Customer', 'WROTE', 'Review'): 77071, ('Product', 'SOLD_BY', 'Brand'): 5555, ('Product', 'rev_REVIEW_OF', 'Review'): 77071, ('Review', 'REVIEW_OF', 'Product'): 77071, ('Review', 'rev_WROTE', 'Customer'): 77071},\n",
       "      metagraph=[('Brand', 'Product', 'rev_SOLD_BY'), ('Product', 'Brand', 'SOLD_BY'), ('Product', 'Review', 'rev_REVIEW_OF'), ('Customer', 'Review', 'WROTE'), ('Review', 'Product', 'REVIEW_OF'), ('Review', 'Customer', 'rev_WROTE')])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = dgl.load_graphs(\"./graphs/industrial_and_scientific_5_core.dgl\")[0][0]\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RGCN({x: g.number_of_nodes(x) for x in g.ntypes}, 512, 256, 2, g.etypes)\n",
    "\n",
    "labels = g.nodes[\"Review\"].data[\"Positive\"]\n",
    "train_mask = g.nodes[\"Review\"].data[\"train_mask\"]\n",
    "test_mask = g.nodes[\"Review\"].data[\"test_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 0.6928762793540955\n",
      "Epoch: 1 Loss: 0.6704129576683044\n",
      "Epoch: 2 Loss: 0.64774489402771\n",
      "Epoch: 3 Loss: 0.621573805809021\n",
      "Epoch: 4 Loss: 0.5905534625053406\n",
      "Epoch: 5 Loss: 0.5541093349456787\n",
      "Epoch: 6 Loss: 0.5124903917312622\n",
      "Epoch: 7 Loss: 0.4669405519962311\n",
      "Epoch: 8 Loss: 0.41965290904045105\n",
      "Epoch: 9 Loss: 0.37351658940315247\n",
      "Epoch: 10 Loss: 0.3315752148628235\n",
      "Epoch: 11 Loss: 0.29633381962776184\n",
      "Epoch: 12 Loss: 0.26914796233177185\n",
      "Epoch: 13 Loss: 0.24983493983745575\n",
      "Epoch: 14 Loss: 0.2367929071187973\n",
      "Epoch: 15 Loss: 0.22761522233486176\n",
      "Epoch: 16 Loss: 0.21990403532981873\n",
      "Epoch: 17 Loss: 0.21187925338745117\n",
      "Epoch: 18 Loss: 0.20263557136058807\n",
      "Epoch: 19 Loss: 0.19210666418075562\n",
      "Epoch: 20 Loss: 0.18078264594078064\n",
      "Epoch: 21 Loss: 0.16943512856960297\n",
      "Epoch: 22 Loss: 0.1588880568742752\n",
      "Epoch: 23 Loss: 0.1498504877090454\n",
      "Epoch: 24 Loss: 0.1428607851266861\n",
      "Epoch: 25 Loss: 0.13811862468719482\n",
      "Epoch: 26 Loss: 0.13531246781349182\n",
      "Epoch: 27 Loss: 0.1337178349494934\n",
      "Epoch: 28 Loss: 0.13246998190879822\n",
      "Epoch: 29 Loss: 0.13083940744400024\n",
      "Epoch: 30 Loss: 0.12846492230892181\n",
      "Epoch: 31 Loss: 0.12538692355155945\n",
      "Epoch: 32 Loss: 0.1219274178147316\n",
      "Epoch: 33 Loss: 0.11852682381868362\n",
      "Epoch: 34 Loss: 0.11558011919260025\n",
      "Epoch: 35 Loss: 0.11332514137029648\n",
      "Epoch: 36 Loss: 0.11179942637681961\n",
      "Epoch: 37 Loss: 0.11086789518594742\n",
      "Epoch: 38 Loss: 0.11029163002967834\n",
      "Epoch: 39 Loss: 0.10981183499097824\n",
      "Epoch: 40 Loss: 0.10922130197286606\n",
      "Epoch: 41 Loss: 0.10840409994125366\n",
      "Epoch: 42 Loss: 0.10734587162733078\n",
      "Epoch: 43 Loss: 0.10611842572689056\n",
      "Epoch: 44 Loss: 0.10484891384840012\n",
      "Epoch: 45 Loss: 0.10367937386035919\n",
      "Epoch: 46 Loss: 0.1027241125702858\n",
      "Epoch: 47 Loss: 0.10203666985034943\n",
      "Epoch: 48 Loss: 0.10159391164779663\n",
      "Epoch: 49 Loss: 0.10130588710308075\n",
      "Epoch: 50 Loss: 0.1010521799325943\n",
      "Epoch: 51 Loss: 0.10072941333055496\n",
      "Epoch: 52 Loss: 0.10028894245624542\n",
      "Epoch: 53 Loss: 0.09974858909845352\n",
      "Epoch: 54 Loss: 0.09917332977056503\n",
      "Epoch: 55 Loss: 0.0986405462026596\n",
      "Epoch: 56 Loss: 0.0982065424323082\n",
      "Epoch: 57 Loss: 0.09788695722818375\n",
      "Epoch: 58 Loss: 0.09765854477882385\n",
      "Epoch: 59 Loss: 0.09747433662414551\n",
      "Epoch: 60 Loss: 0.09728491306304932\n",
      "Epoch: 61 Loss: 0.09705910831689835\n",
      "Epoch: 62 Loss: 0.09679298102855682\n",
      "Epoch: 63 Loss: 0.09650649130344391\n",
      "Epoch: 64 Loss: 0.09623131901025772\n",
      "Epoch: 65 Loss: 0.09599500149488449\n",
      "Epoch: 66 Loss: 0.09580874443054199\n",
      "Epoch: 67 Loss: 0.09566393494606018\n",
      "Epoch: 68 Loss: 0.09553878009319305\n",
      "Epoch: 69 Loss: 0.09541012346744537\n",
      "Epoch: 70 Loss: 0.09526414424180984\n",
      "Epoch: 71 Loss: 0.09510165452957153\n",
      "Epoch: 72 Loss: 0.09493572264909744\n",
      "Epoch: 73 Loss: 0.09478318691253662\n",
      "Epoch: 74 Loss: 0.09465499222278595\n",
      "Epoch: 75 Loss: 0.0945511981844902\n",
      "Epoch: 76 Loss: 0.09446254372596741\n",
      "Epoch: 77 Loss: 0.0943765640258789\n",
      "Epoch: 78 Loss: 0.09428416192531586\n",
      "Epoch: 79 Loss: 0.09418365359306335\n",
      "Epoch: 80 Loss: 0.09408005326986313\n",
      "Epoch: 81 Loss: 0.09398087114095688\n",
      "Epoch: 82 Loss: 0.0938916727900505\n",
      "Epoch: 83 Loss: 0.09381341934204102\n",
      "Epoch: 84 Loss: 0.0937424749135971\n",
      "Epoch: 85 Loss: 0.09367356449365616\n",
      "Epoch: 86 Loss: 0.0936034694314003\n",
      "Epoch: 87 Loss: 0.09353272616863251\n",
      "Epoch: 88 Loss: 0.09346450120210648\n",
      "Epoch: 89 Loss: 0.0934017226099968\n",
      "Epoch: 90 Loss: 0.09334491193294525\n",
      "Epoch: 91 Loss: 0.09329209476709366\n",
      "Epoch: 92 Loss: 0.09324038028717041\n",
      "Epoch: 93 Loss: 0.0931883454322815\n",
      "Epoch: 94 Loss: 0.09313642978668213\n",
      "Epoch: 95 Loss: 0.0930861309170723\n",
      "Epoch: 96 Loss: 0.09303884953260422\n",
      "Epoch: 97 Loss: 0.09299501031637192\n",
      "Epoch: 98 Loss: 0.09295365959405899\n",
      "Epoch: 99 Loss: 0.09291335940361023\n",
      "Epoch: 100 Loss: 0.09287315607070923\n",
      "Epoch: 101 Loss: 0.09283293783664703\n",
      "Epoch: 102 Loss: 0.09279339760541916\n",
      "Epoch: 103 Loss: 0.09275532513856888\n",
      "Epoch: 104 Loss: 0.09271881729364395\n",
      "Epoch: 105 Loss: 0.09268311411142349\n",
      "Epoch: 106 Loss: 0.09264691919088364\n",
      "Epoch: 107 Loss: 0.09261000901460648\n",
      "Epoch: 108 Loss: 0.09257283806800842\n",
      "Epoch: 109 Loss: 0.09253649413585663\n",
      "Epoch: 110 Loss: 0.09250173717737198\n",
      "Epoch: 111 Loss: 0.09246868640184402\n",
      "Epoch: 112 Loss: 0.09243661165237427\n",
      "Epoch: 113 Loss: 0.09240434318780899\n",
      "Epoch: 114 Loss: 0.09237010031938553\n",
      "Epoch: 115 Loss: 0.09233315289020538\n",
      "Epoch: 116 Loss: 0.09229432046413422\n",
      "Epoch: 117 Loss: 0.09225456416606903\n",
      "Epoch: 118 Loss: 0.09221339970827103\n",
      "Epoch: 119 Loss: 0.09217113256454468\n",
      "Epoch: 120 Loss: 0.09212891012430191\n",
      "Epoch: 121 Loss: 0.09208625555038452\n",
      "Epoch: 122 Loss: 0.09204448014497757\n",
      "Epoch: 123 Loss: 0.09200437366962433\n",
      "Epoch: 124 Loss: 0.09196549654006958\n",
      "Epoch: 125 Loss: 0.0919269546866417\n",
      "Epoch: 126 Loss: 0.09188783913850784\n",
      "Epoch: 127 Loss: 0.09184779971837997\n",
      "Epoch: 128 Loss: 0.0918072760105133\n",
      "Epoch: 129 Loss: 0.09176761656999588\n",
      "Epoch: 130 Loss: 0.09172970801591873\n",
      "Epoch: 131 Loss: 0.09169305115938187\n",
      "Epoch: 132 Loss: 0.09165621548891068\n",
      "Epoch: 133 Loss: 0.09161738306283951\n",
      "Epoch: 134 Loss: 0.09157634526491165\n",
      "Epoch: 135 Loss: 0.09153382480144501\n",
      "Epoch: 136 Loss: 0.09149053692817688\n",
      "Epoch: 137 Loss: 0.09144619107246399\n",
      "Epoch: 138 Loss: 0.09140066802501678\n",
      "Epoch: 139 Loss: 0.0913548469543457\n",
      "Epoch: 140 Loss: 0.09131031483411789\n",
      "Epoch: 141 Loss: 0.09126780182123184\n",
      "Epoch: 142 Loss: 0.09122689068317413\n",
      "Epoch: 143 Loss: 0.09118662029504776\n",
      "Epoch: 144 Loss: 0.09114664793014526\n",
      "Epoch: 145 Loss: 0.09110675007104874\n",
      "Epoch: 146 Loss: 0.0910673439502716\n",
      "Epoch: 147 Loss: 0.09102901816368103\n",
      "Epoch: 148 Loss: 0.09099170565605164\n",
      "Epoch: 149 Loss: 0.09095510095357895\n",
      "Epoch: 150 Loss: 0.09091924875974655\n",
      "Epoch: 151 Loss: 0.0908845067024231\n",
      "Epoch: 152 Loss: 0.09085170924663544\n",
      "Epoch: 153 Loss: 0.09082088619470596\n",
      "Epoch: 154 Loss: 0.09079141169786453\n",
      "Epoch: 155 Loss: 0.09076254814863205\n",
      "Epoch: 156 Loss: 0.090734101831913\n",
      "Epoch: 157 Loss: 0.09070629626512527\n",
      "Epoch: 158 Loss: 0.09067942947149277\n",
      "Epoch: 159 Loss: 0.0906536653637886\n",
      "Epoch: 160 Loss: 0.09062893688678741\n",
      "Epoch: 161 Loss: 0.09060503542423248\n",
      "Epoch: 162 Loss: 0.09058194607496262\n",
      "Epoch: 163 Loss: 0.09055966883897781\n",
      "Epoch: 164 Loss: 0.09053830057382584\n",
      "Epoch: 165 Loss: 0.09051792323589325\n",
      "Epoch: 166 Loss: 0.09049838781356812\n",
      "Epoch: 167 Loss: 0.09047940373420715\n",
      "Epoch: 168 Loss: 0.0904608741402626\n",
      "Epoch: 169 Loss: 0.09044284373521805\n",
      "Epoch: 170 Loss: 0.09042549133300781\n",
      "Epoch: 171 Loss: 0.09040890634059906\n",
      "Epoch: 172 Loss: 0.09039300680160522\n",
      "Epoch: 173 Loss: 0.09037758409976959\n",
      "Epoch: 174 Loss: 0.0903625413775444\n",
      "Epoch: 175 Loss: 0.0903477892279625\n",
      "Epoch: 176 Loss: 0.09033340960741043\n",
      "Epoch: 177 Loss: 0.09031939506530762\n",
      "Epoch: 178 Loss: 0.09030577540397644\n",
      "Epoch: 179 Loss: 0.09029249846935272\n",
      "Epoch: 180 Loss: 0.09027949720621109\n",
      "Epoch: 181 Loss: 0.09026671200990677\n",
      "Epoch: 182 Loss: 0.09025415778160095\n",
      "Epoch: 183 Loss: 0.09024180471897125\n",
      "Epoch: 184 Loss: 0.0902295708656311\n",
      "Epoch: 185 Loss: 0.09021739661693573\n",
      "Epoch: 186 Loss: 0.0902053490281105\n",
      "Epoch: 187 Loss: 0.0901934951543808\n",
      "Epoch: 188 Loss: 0.09018190205097198\n",
      "Epoch: 189 Loss: 0.09017075598239899\n",
      "Epoch: 190 Loss: 0.09016013890504837\n",
      "Epoch: 191 Loss: 0.09015002101659775\n",
      "Epoch: 192 Loss: 0.09014027565717697\n",
      "Epoch: 193 Loss: 0.09013067185878754\n",
      "Epoch: 194 Loss: 0.09012110531330109\n",
      "Epoch: 195 Loss: 0.09011159092187881\n",
      "Epoch: 196 Loss: 0.09010222554206848\n",
      "Epoch: 197 Loss: 0.09009312093257904\n",
      "Epoch: 198 Loss: 0.09008431434631348\n",
      "Epoch: 199 Loss: 0.09007581323385239\n",
      "Epoch: 200 Loss: 0.0900675505399704\n",
      "Epoch: 201 Loss: 0.09005945920944214\n",
      "Epoch: 202 Loss: 0.09005144983530045\n",
      "Epoch: 203 Loss: 0.09004352986812592\n",
      "Epoch: 204 Loss: 0.09003567695617676\n",
      "Epoch: 205 Loss: 0.09002792090177536\n",
      "Epoch: 206 Loss: 0.09002024680376053\n",
      "Epoch: 207 Loss: 0.09001269191503525\n",
      "Epoch: 208 Loss: 0.09000526368618011\n",
      "Epoch: 209 Loss: 0.08999799191951752\n",
      "Epoch: 210 Loss: 0.08999085426330566\n",
      "Epoch: 211 Loss: 0.08998389542102814\n",
      "Epoch: 212 Loss: 0.08997706323862076\n",
      "Epoch: 213 Loss: 0.08997037261724472\n",
      "Epoch: 214 Loss: 0.08996379375457764\n",
      "Epoch: 215 Loss: 0.08995731920003891\n",
      "Epoch: 216 Loss: 0.08995093405246735\n",
      "Epoch: 217 Loss: 0.08994463831186295\n",
      "Epoch: 218 Loss: 0.08993840962648392\n",
      "Epoch: 219 Loss: 0.08993227034807205\n",
      "Epoch: 220 Loss: 0.08992622792720795\n",
      "Epoch: 221 Loss: 0.08992023020982742\n",
      "Epoch: 222 Loss: 0.08991433680057526\n",
      "Epoch: 223 Loss: 0.08990850299596786\n",
      "Epoch: 224 Loss: 0.08990274369716644\n",
      "Epoch: 225 Loss: 0.08989707380533218\n",
      "Epoch: 226 Loss: 0.08989150077104568\n",
      "Epoch: 227 Loss: 0.08988603204488754\n",
      "Epoch: 228 Loss: 0.08988063782453537\n",
      "Epoch: 229 Loss: 0.08987531065940857\n",
      "Epoch: 230 Loss: 0.08987004309892654\n",
      "Epoch: 231 Loss: 0.0898648276925087\n",
      "Epoch: 232 Loss: 0.08985964208841324\n",
      "Epoch: 233 Loss: 0.0898544043302536\n",
      "Epoch: 234 Loss: 0.08984897285699844\n",
      "Epoch: 235 Loss: 0.08984306454658508\n",
      "Epoch: 236 Loss: 0.08983668684959412\n",
      "Epoch: 237 Loss: 0.08983049541711807\n",
      "Epoch: 238 Loss: 0.08982528001070023\n",
      "Epoch: 239 Loss: 0.08982086181640625\n",
      "Epoch: 240 Loss: 0.08981622755527496\n",
      "Epoch: 241 Loss: 0.08981107175350189\n",
      "Epoch: 242 Loss: 0.08980575948953629\n",
      "Epoch: 243 Loss: 0.08980082720518112\n",
      "Epoch: 244 Loss: 0.08979643881320953\n",
      "Epoch: 245 Loss: 0.08979228883981705\n",
      "Epoch: 246 Loss: 0.08978792279958725\n",
      "Epoch: 247 Loss: 0.08978325873613358\n",
      "Epoch: 248 Loss: 0.0897785872220993\n",
      "Epoch: 249 Loss: 0.08977418392896652\n",
      "Epoch: 250 Loss: 0.08977004140615463\n",
      "Epoch: 251 Loss: 0.08976592123508453\n",
      "Epoch: 252 Loss: 0.08976167440414429\n",
      "Epoch: 253 Loss: 0.08975733816623688\n",
      "Epoch: 254 Loss: 0.08975309133529663\n",
      "Epoch: 255 Loss: 0.08974901586771011\n",
      "Epoch: 256 Loss: 0.08974509686231613\n",
      "Epoch: 257 Loss: 0.08974119275808334\n",
      "Epoch: 258 Loss: 0.08973725140094757\n",
      "Epoch: 259 Loss: 0.0897333174943924\n",
      "Epoch: 260 Loss: 0.08972945809364319\n",
      "Epoch: 261 Loss: 0.08972568809986115\n",
      "Epoch: 262 Loss: 0.08972202986478806\n",
      "Epoch: 263 Loss: 0.08971841633319855\n",
      "Epoch: 264 Loss: 0.08971483260393143\n",
      "Epoch: 265 Loss: 0.0897112786769867\n",
      "Epoch: 266 Loss: 0.08970777690410614\n",
      "Epoch: 267 Loss: 0.08970434218645096\n",
      "Epoch: 268 Loss: 0.08970098197460175\n",
      "Epoch: 269 Loss: 0.08969766646623611\n",
      "Epoch: 270 Loss: 0.08969441801309586\n",
      "Epoch: 271 Loss: 0.08969119936227798\n",
      "Epoch: 272 Loss: 0.0896880254149437\n",
      "Epoch: 273 Loss: 0.0896848812699318\n",
      "Epoch: 274 Loss: 0.08968180418014526\n",
      "Epoch: 275 Loss: 0.0896787941455841\n",
      "Epoch: 276 Loss: 0.08967581391334534\n",
      "Epoch: 277 Loss: 0.08967288583517075\n",
      "Epoch: 278 Loss: 0.08966998755931854\n",
      "Epoch: 279 Loss: 0.08966712653636932\n",
      "Epoch: 280 Loss: 0.08966431766748428\n",
      "Epoch: 281 Loss: 0.08966153860092163\n",
      "Epoch: 282 Loss: 0.08965883404016495\n",
      "Epoch: 283 Loss: 0.08965618908405304\n",
      "Epoch: 284 Loss: 0.08965356647968292\n",
      "Epoch: 285 Loss: 0.08965101093053818\n",
      "Epoch: 286 Loss: 0.08964848518371582\n",
      "Epoch: 287 Loss: 0.08964602649211884\n",
      "Epoch: 288 Loss: 0.08964361250400543\n",
      "Epoch: 289 Loss: 0.08964124321937561\n",
      "Epoch: 290 Loss: 0.08963891118764877\n",
      "Epoch: 291 Loss: 0.08963660150766373\n",
      "Epoch: 292 Loss: 0.08963432908058167\n",
      "Epoch: 293 Loss: 0.08963209390640259\n",
      "Epoch: 294 Loss: 0.0896298810839653\n",
      "Epoch: 295 Loss: 0.089627705514431\n",
      "Epoch: 296 Loss: 0.08962555974721909\n",
      "Epoch: 297 Loss: 0.08962345123291016\n",
      "Epoch: 298 Loss: 0.08962138742208481\n",
      "Epoch: 299 Loss: 0.08961934596300125\n",
      "Epoch: 300 Loss: 0.08961734175682068\n",
      "Epoch: 301 Loss: 0.08961537480354309\n",
      "Epoch: 302 Loss: 0.0896134227514267\n",
      "Epoch: 303 Loss: 0.08961150050163269\n",
      "Epoch: 304 Loss: 0.08960960805416107\n",
      "Epoch: 305 Loss: 0.08960774540901184\n",
      "Epoch: 306 Loss: 0.0896059051156044\n",
      "Epoch: 307 Loss: 0.08960409462451935\n",
      "Epoch: 308 Loss: 0.08960229903459549\n",
      "Epoch: 309 Loss: 0.08960054069757462\n",
      "Epoch: 310 Loss: 0.08959881216287613\n",
      "Epoch: 311 Loss: 0.08959709107875824\n",
      "Epoch: 312 Loss: 0.08959540724754333\n",
      "Epoch: 313 Loss: 0.08959374576807022\n",
      "Epoch: 314 Loss: 0.0895920917391777\n",
      "Epoch: 315 Loss: 0.08959048241376877\n",
      "Epoch: 316 Loss: 0.08958888053894043\n",
      "Epoch: 317 Loss: 0.08958730846643448\n",
      "Epoch: 318 Loss: 0.08958575874567032\n",
      "Epoch: 319 Loss: 0.08958423882722855\n",
      "Epoch: 320 Loss: 0.08958272635936737\n",
      "Epoch: 321 Loss: 0.08958122879266739\n",
      "Epoch: 322 Loss: 0.0895797610282898\n",
      "Epoch: 323 Loss: 0.0895783007144928\n",
      "Epoch: 324 Loss: 0.08957687765359879\n",
      "Epoch: 325 Loss: 0.08957545459270477\n",
      "Epoch: 326 Loss: 0.08957406133413315\n",
      "Epoch: 327 Loss: 0.08957269042730331\n",
      "Epoch: 328 Loss: 0.08957133442163467\n",
      "Epoch: 329 Loss: 0.08956999331712723\n",
      "Epoch: 330 Loss: 0.08956868201494217\n",
      "Epoch: 331 Loss: 0.08956737816333771\n",
      "Epoch: 332 Loss: 0.08956609666347504\n",
      "Epoch: 333 Loss: 0.08956483006477356\n",
      "Epoch: 334 Loss: 0.08956357091665268\n",
      "Epoch: 335 Loss: 0.08956234157085419\n",
      "Epoch: 336 Loss: 0.08956111967563629\n",
      "Epoch: 337 Loss: 0.08955991268157959\n",
      "Epoch: 338 Loss: 0.08955873548984528\n",
      "Epoch: 339 Loss: 0.08955755829811096\n",
      "Epoch: 340 Loss: 0.08955640345811844\n",
      "Epoch: 341 Loss: 0.08955525606870651\n",
      "Epoch: 342 Loss: 0.08955412358045578\n",
      "Epoch: 343 Loss: 0.08955301344394684\n",
      "Epoch: 344 Loss: 0.08955191820859909\n",
      "Epoch: 345 Loss: 0.08955083042383194\n",
      "Epoch: 346 Loss: 0.08954975754022598\n",
      "Epoch: 347 Loss: 0.08954869210720062\n",
      "Epoch: 348 Loss: 0.08954764902591705\n",
      "Epoch: 349 Loss: 0.08954662829637527\n",
      "Epoch: 350 Loss: 0.0895456001162529\n",
      "Epoch: 351 Loss: 0.08954460173845291\n",
      "Epoch: 352 Loss: 0.08954360336065292\n",
      "Epoch: 353 Loss: 0.08954261988401413\n",
      "Epoch: 354 Loss: 0.08954164385795593\n",
      "Epoch: 355 Loss: 0.08954069763422012\n",
      "Epoch: 356 Loss: 0.08953973650932312\n",
      "Epoch: 357 Loss: 0.0895388051867485\n",
      "Epoch: 358 Loss: 0.08953788131475449\n",
      "Epoch: 359 Loss: 0.08953697234392166\n",
      "Epoch: 360 Loss: 0.08953606337308884\n",
      "Epoch: 361 Loss: 0.0895351767539978\n",
      "Epoch: 362 Loss: 0.08953429758548737\n",
      "Epoch: 363 Loss: 0.08953342586755753\n",
      "Epoch: 364 Loss: 0.08953256905078888\n",
      "Epoch: 365 Loss: 0.08953171968460083\n",
      "Epoch: 366 Loss: 0.08953087776899338\n",
      "Epoch: 367 Loss: 0.08953003585338593\n",
      "Epoch: 368 Loss: 0.08952922374010086\n",
      "Epoch: 369 Loss: 0.0895284116268158\n",
      "Epoch: 370 Loss: 0.08952761441469193\n",
      "Epoch: 371 Loss: 0.08952681720256805\n",
      "Epoch: 372 Loss: 0.08952602744102478\n",
      "Epoch: 373 Loss: 0.0895252525806427\n",
      "Epoch: 374 Loss: 0.08952449262142181\n",
      "Epoch: 375 Loss: 0.08952372521162033\n",
      "Epoch: 376 Loss: 0.08952296525239944\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/parkererickson/nlp8980/final_project/gnn.ipynb Cell 4'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/parkererickson/nlp8980/final_project/gnn.ipynb#ch0000013?line=6'>7</a>\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mcross_entropy(logits[train_mask \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m], labels[train_mask \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mtype(torch\u001b[39m.\u001b[39mlong))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/parkererickson/nlp8980/final_project/gnn.ipynb#ch0000013?line=7'>8</a>\u001b[0m opt\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/parkererickson/nlp8980/final_project/gnn.ipynb#ch0000013?line=8'>9</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/parkererickson/nlp8980/final_project/gnn.ipynb#ch0000013?line=9'>10</a>\u001b[0m opt\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/parkererickson/nlp8980/final_project/gnn.ipynb#ch0000013?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEpoch:\u001b[39m\u001b[39m\"\u001b[39m, epoch, \u001b[39m\"\u001b[39m\u001b[39mLoss:\u001b[39m\u001b[39m\"\u001b[39m, loss\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[0;32m/opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/_tensor.py?line=297'>298</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/_tensor.py?line=298'>299</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/_tensor.py?line=299'>300</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/_tensor.py?line=300'>301</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/_tensor.py?line=304'>305</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/_tensor.py?line=305'>306</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/_tensor.py?line=306'>307</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/autograd/__init__.py?line=150'>151</a>\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/autograd/__init__.py?line=151'>152</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m--> <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/autograd/__init__.py?line=153'>154</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/autograd/__init__.py?line=154'>155</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/autograd/__init__.py?line=155'>156</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/autograd/function.py:189\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/autograd/function.py?line=187'>188</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mBackwardCFunction\u001b[39;00m(_C\u001b[39m.\u001b[39m_FunctionBase, FunctionCtx, _HookMixin):\n\u001b[0;32m--> <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/autograd/function.py?line=188'>189</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs):\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/autograd/function.py?line=189'>190</a>\u001b[0m         \u001b[39m# _forward_cls is defined by derived class\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/autograd/function.py?line=190'>191</a>\u001b[0m         \u001b[39m# The user should define either backward or vjp but never both.\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/autograd/function.py?line=191'>192</a>\u001b[0m         backward_fn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_cls\u001b[39m.\u001b[39mbackward  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/autograd/function.py?line=192'>193</a>\u001b[0m         vjp_fn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_cls\u001b[39m.\u001b[39mvjp  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "opt = torch.optim.Adam(model.parameters())\n",
    "\n",
    "for epoch in range(250):\n",
    "    model.train()\n",
    "\n",
    "    logits = model(g)[\"Review\"]\n",
    "    loss = F.cross_entropy(logits[train_mask == 1], labels[train_mask == 1].type(torch.long))\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    print(\"Epoch:\", epoch, \"Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model(g)[\"Review\"][test_mask == 1].detach().numpy()\n",
    "y_test = labels[test_mask == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.24      0.24      0.24      4750\n",
      "         1.0       0.89      0.89      0.89     33786\n",
      "\n",
      "    accuracy                           0.81     38536\n",
      "   macro avg       0.56      0.56      0.56     38536\n",
      "weighted avg       0.81      0.81      0.81     38536\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, preds.argmax(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.892437472250999"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_test, preds.argmax(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7f0fb73b74445d6392909c9d469b21cbfa1ec308c4aeb674f6b2b586eed9f638"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('huggingface')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
