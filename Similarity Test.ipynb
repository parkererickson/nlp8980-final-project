{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89c001be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[603, 14747, 18813, 21401, 35713, 38271, 41986, 41987, 43997, 55969, 56410]\n"
     ]
    }
   ],
   "source": [
    "import cl_graph_bert as cgm\n",
    "import torch\n",
    "from torch import nn\n",
    "import json\n",
    "from transformers import BertModel, BertConfig, BertTokenizer, AdamW\n",
    "import tqdm\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class ReviewDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokens, labels, ids):\n",
    "        self.tokens = tokens\n",
    "        self.labels = labels\n",
    "        self.ids = ids\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.tokens['input_ids'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.tokens.items()}\n",
    "        out = {\"tokens\": item, \"label\": self.labels[idx], \"id\": self.ids[idx]}\n",
    "        return out\n",
    "\n",
    "def process_text(filepath, batch_size):\n",
    "    reviews = []\n",
    "    data = open(filepath)\n",
    "    for line in data.readlines():\n",
    "        reviews.append(json.loads(line))\n",
    "\n",
    "    review_texts = []\n",
    "    review_scores = []\n",
    "    missing = []\n",
    "    i = 0\n",
    "    for sample in reviews:\n",
    "        if 'reviewText' in sample and 'overall' in sample:\n",
    "            review_texts.append(sample['reviewText'])\n",
    "            if sample['overall'] >= 4:\n",
    "                review_scores.append(1)\n",
    "            else:\n",
    "                review_scores.append(0)\n",
    "        else:\n",
    "            review_texts.append(\"NULL\")\n",
    "            missing.append(i)\n",
    "            \n",
    "        i += 1\n",
    "\n",
    "     \n",
    "            \n",
    "    print(missing)            \n",
    "    train_reviews = review_texts[:len(review_texts)//2]\n",
    "    train_ids = [i for i in range(0, len(review_texts)//2+1)]\n",
    "    test_reviews = review_texts[len(review_texts)//2:]\n",
    "    test_ids = [i for i in range(len(review_texts)//2, len(review_texts))]\n",
    "    train_scores = review_scores[:len(review_texts)//2]\n",
    "    test_scores = review_scores[len(review_texts)//2:]\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    tokenized_train_reviews = tokenizer(train_reviews, return_tensors=\"pt\", padding='max_length', truncation=True)\n",
    "    tokenized_test_reviews = tokenizer(test_reviews, return_tensors=\"pt\", padding='max_length', truncation=True)\n",
    "\n",
    "    tokenized_reviews = tokenizer(review_texts, return_tensors=\"pt\", padding='max_length', truncation=True)\n",
    "    \n",
    "    train_dataset = ReviewDataset(tokenized_train_reviews, train_scores, train_ids)\n",
    "    test_dataset = ReviewDataset(tokenized_test_reviews, test_scores, test_ids)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "    return train_loader, test_loader, tokenized_reviews\n",
    "\n",
    "# %%\n",
    "batch_size = 16\n",
    "train_loader, test_loader, review_tokens = process_text(\"../review_dataset/Industrial_and_Scientific_5.json\", batch_size)\n",
    "\n",
    "# %%\n",
    "import dgl\n",
    "g = dgl.load_graphs(\"./graphs/industrial_and_scientific_5_core.dgl\")[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e361a125",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIPGraphModel(\n",
       "  (graph_model): RGCN(\n",
       "    (embed): ParameterDict(\n",
       "        (Brand): Parameter containing: [torch.FloatTensor of size 1900x512]\n",
       "        (Customer): Parameter containing: [torch.FloatTensor of size 11041x512]\n",
       "        (Product): Parameter containing: [torch.FloatTensor of size 5334x512]\n",
       "        (Review): Parameter containing: [torch.FloatTensor of size 77071x512]\n",
       "    )\n",
       "    (conv1): HeteroGraphConv(\n",
       "      (mods): ModuleDict(\n",
       "        (rev_SOLD_BY): GraphConv(in=512, out=256, normalization=both, activation=None)\n",
       "        (WROTE): GraphConv(in=512, out=256, normalization=both, activation=None)\n",
       "        (SOLD_BY): GraphConv(in=512, out=256, normalization=both, activation=None)\n",
       "        (rev_REVIEW_OF): GraphConv(in=512, out=256, normalization=both, activation=None)\n",
       "        (REVIEW_OF): GraphConv(in=512, out=256, normalization=both, activation=None)\n",
       "        (rev_WROTE): GraphConv(in=512, out=256, normalization=both, activation=None)\n",
       "      )\n",
       "    )\n",
       "    (conv2): HeteroGraphConv(\n",
       "      (mods): ModuleDict(\n",
       "        (rev_SOLD_BY): GraphConv(in=256, out=256, normalization=both, activation=None)\n",
       "        (WROTE): GraphConv(in=256, out=256, normalization=both, activation=None)\n",
       "        (SOLD_BY): GraphConv(in=256, out=256, normalization=both, activation=None)\n",
       "        (rev_REVIEW_OF): GraphConv(in=256, out=256, normalization=both, activation=None)\n",
       "        (REVIEW_OF): GraphConv(in=256, out=256, normalization=both, activation=None)\n",
       "        (rev_WROTE): GraphConv(in=256, out=256, normalization=both, activation=None)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (graph_projection): Projection(\n",
       "    (projection): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (relu): ReLU()\n",
       "    (fc): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (language_model): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (language_projection): Projection(\n",
       "    (projection): Linear(in_features=768, out_features=512, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (relu): ReLU()\n",
       "    (fc): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = cgm.CLIPGraphModel(\n",
    "    rel_types = g.etypes,\n",
    "    emb_types = {x: g.number_of_nodes(x) for x in g.ntypes} \n",
    ")\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "model.load_state_dict(torch.load(\"./base_statedict_6668.011407389138.pt\", map_location=torch.device(device)))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a37cedee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes={'Brand': 1900, 'Customer': 11041, 'Product': 5334, 'Review': 77071},\n",
       "      num_edges={('Brand', 'rev_SOLD_BY', 'Product'): 5555, ('Customer', 'WROTE', 'Review'): 77071, ('Product', 'SOLD_BY', 'Brand'): 5555, ('Product', 'rev_REVIEW_OF', 'Review'): 77071, ('Review', 'REVIEW_OF', 'Product'): 77071, ('Review', 'rev_WROTE', 'Customer'): 77071},\n",
       "      metagraph=[('Brand', 'Product', 'rev_SOLD_BY'), ('Product', 'Brand', 'SOLD_BY'), ('Product', 'Review', 'rev_REVIEW_OF'), ('Customer', 'Review', 'WROTE'), ('Review', 'Product', 'REVIEW_OF'), ('Review', 'Customer', 'rev_WROTE')])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a062d2cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([77071, 512])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_tokens.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "179e21ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_cuda(tokens, device):\n",
    "    dictionary = {}\n",
    "    for key, value in tokens.items():\n",
    "        dictionary[key] = value.to(device)\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a149086f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5334/5334 [14:02<00:00,  6.33it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np \n",
    "import tqdm\n",
    "\n",
    "bert_base = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "bert_base.to('cuda')\n",
    "model.language_model.to('cuda')\n",
    "\n",
    "reivew_ids = []\n",
    "\n",
    "gnn_sims = []\n",
    "bert_base_sims = []\n",
    "bert_finetuned_sims = []\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for i in tqdm.tqdm(range(g.num_nodes('Product'))):\n",
    "        review_id = g.successors(i, 'rev_REVIEW_OF')\n",
    "        \n",
    "        if len(review_id) > 1 and len(review_id) < 650:\n",
    "\n",
    "            reivew_ids.append(review_id)\n",
    "                \n",
    "            tokens = review_tokens.input_ids[review_id].to('cuda')\n",
    "            attentions = review_tokens.attention_mask[review_id].to('cuda')\n",
    "            token_types = review_tokens.token_type_ids[review_id].to('cuda')\n",
    "\n",
    "            gnn_embs = model.graph_model.embed[\"Review\"][review_id].detach().numpy()\n",
    "            \n",
    "            base_bert_embs = bert_base(input_ids = tokens, \n",
    "                 attention_mask= attentions, \n",
    "                 token_type_ids= token_types).last_hidden_state[:,0].detach().cpu().numpy()\n",
    "            \n",
    "            bert_finetuned_embs = model.language_model(input_ids = tokens, \n",
    "                 attention_mask= attentions, \n",
    "                 token_type_ids= token_types).last_hidden_state[:,0].detach().cpu().numpy()\n",
    "            \n",
    "            gnn_sims.append( (np.sum(cosine_similarity(gnn_embs)) - len(review_id)) /  (len(review_id) ** 2 - len(review_id)) )\n",
    "            bert_base_sims.append( (np.sum(cosine_similarity(base_bert_embs)) - len(review_id)) /  (len(review_id) ** 2 - len(review_id)) )\n",
    "            bert_finetuned_sims.append( (np.sum(cosine_similarity(bert_finetuned_embs)) - len(review_id)) / (len(review_id) ** 2 - len(review_id)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ff94f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7741479140061599"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "review_id = g.successors(0, 'rev_REVIEW_OF')\n",
    "\n",
    "embs = model.graph_model.embed[\"Review\"][review_id].detach().numpy()\n",
    "(np.sum(cosine_similarity(embs)) - 13) / (13*13 - 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34b20d22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8752829233805338"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_base.to('cpu')\n",
    "embs = bert_base(input_ids = review_tokens.input_ids[review_id], \n",
    "                 attention_mask= review_tokens.attention_mask[review_id], \n",
    "                 token_type_ids= review_tokens.token_type_ids[review_id]).last_hidden_state[:,0].detach().numpy()\n",
    "(np.sum(cosine_similarity(embs)) - 13) / (12*12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0f89cd61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmin(cosine_similarity(embs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d258c1ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "305"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(review_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3d6e6d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0,  1023,  3244,  3429,  4197,  5451,  5550, 21650, 21807, 58913])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.successors(0, 'WROTE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7bf1720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4351822747124566 0.7986572265625 1.0\n",
      "0.6400400161743164 0.8430681228637695 1.0\n",
      "0.6438650131225586 0.8048208872477214 0.9999998728434245\n",
      "0.2493367936876085 0.7941697014702691 1.0\n",
      "0.1823503017425537 0.8022414207458496 1.0\n",
      "0.26235570907592776 0.7868319511413574 1.0\n",
      "0.438725217183431 0.757946523030599 1.0\n",
      "0.5948143674616229 0.7766534459521199 1.0\n",
      "0.5275150934855143 0.762452761332194 1.0\n",
      "0.6180869511195591 0.8213873363676525 1.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(gnn_sims[i], bert_base_sims[i], bert_finetuned_sims[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceba0796",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DefBERT python",
   "language": "python",
   "name": "defbert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
