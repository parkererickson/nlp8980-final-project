{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10d27b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import numpy as np\n",
    "from transformers import BertModel, BertConfig, BertTokenizer, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19c324fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = []\n",
    "data = open(\"./data/Industrial_and_Scientific_5.json\")\n",
    "for line in data.readlines():\n",
    "    reviews.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de540d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_texts = []\n",
    "review_scores = []\n",
    "\n",
    "for sample in reviews:\n",
    "    if 'reviewText' in sample and 'overall' in sample:\n",
    "        review_texts.append(sample['reviewText'])\n",
    "        if sample['overall'] >= 4:\n",
    "            review_scores.append(1)\n",
    "        else:\n",
    "            review_scores.append(0)\n",
    "        \n",
    "train_reviews = review_texts[:len(review_texts)//2]\n",
    "train_ids = [i for i in range(0, len(review_texts)//2+1)]\n",
    "test_reviews = review_texts[len(review_texts)//2:]\n",
    "test_ids = [i for i in range(len(review_texts)//2, len(review_texts))]\n",
    "train_scores = review_scores[:len(review_texts)//2]\n",
    "test_scores = review_scores[len(review_texts)//2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42cec9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "tokenized_train_reviews = tokenizer(train_reviews, return_tensors=\"pt\", padding='max_length', truncation=True)\n",
    "tokenized_test_reviews = tokenizer(test_reviews, return_tensors=\"pt\", padding='max_length', truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7879e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "\n",
    "class ReviewDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokens, labels, ids):\n",
    "        self.tokens = tokens\n",
    "        self.labels = labels\n",
    "        self.ids = ids\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.tokens['input_ids'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.tokens.items()}\n",
    "        out = {\"tokens\": item, \"label\": self.labels[idx], \"id\": self.ids[idx]}\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9009206c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 12\n",
    "\n",
    "train_dataset = ReviewDataset(tokenized_train_reviews, train_scores, train_ids)\n",
    "test_dataset = ReviewDataset(tokenized_test_reviews, test_scores, test_ids)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "358c73cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([32158,  3181,  9502,  8347,  3472, 21376, 13765,  1012, 25064, 34592,\n",
      "        27624, 24796])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m8/dqp9wj910q53mc6120n2z62m0000gn/T/ipykernel_24085/3438568516.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.tokens.items()}\n"
     ]
    }
   ],
   "source": [
    "for i in train_loader:\n",
    "    print(i[\"id\"])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c44716e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertMLP(nn.Module):\n",
    "  \n",
    "    def __init__(self, num_labels=2, hidden_size=100, dropout_prob=.1):\n",
    "        super(BertMLP, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "#         self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(768, num_labels)\n",
    "\n",
    "    def forward(self, batch, return_embs = False):\n",
    "        bert_emb = self.bert(input_ids = batch['input_ids'], \n",
    "           attention_mask=batch['attention_mask'], \n",
    "           token_type_ids=batch['token_type_ids']).last_hidden_state[:,0]\n",
    "        logits = self.classifier(bert_emb)\n",
    "        \n",
    "        if return_embs: \n",
    "            return logits, bert_emb\n",
    "        else:\n",
    "            return logits\n",
    "        \n",
    "def tokens_to_cuda(tokens, device):\n",
    "    dictionary = {}\n",
    "    for key, value in tokens.items():\n",
    "        dictionary[key] = value.to(device)\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e0f2bbc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "model = BertMLP()\n",
    "model.to(device)\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    [\n",
    "        {\"params\":model.bert.parameters(),\"lr\": 0.00001},\n",
    "        {\"params\":model.classifier.parameters(), \"lr\": 0.001},       \n",
    "   ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b700348",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/parkererickson/nlp8980/final_project/bert_finetuning.ipynb Cell 10'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/parkererickson/nlp8980/final_project/bert_finetuning.ipynb#ch0000008?line=0'>1</a>\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/parkererickson/nlp8980/final_project/bert_finetuning.ipynb#ch0000008?line=2'>3</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/parkererickson/nlp8980/final_project/bert_finetuning.ipynb#ch0000008?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39;49m(epochs)):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/parkererickson/nlp8980/final_project/bert_finetuning.ipynb#ch0000008?line=5'>6</a>\u001b[0m     epoch_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/parkererickson/nlp8980/final_project/bert_finetuning.ipynb#ch0000008?line=6'>7</a>\u001b[0m     \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m tqdm(train_loader):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/tqdm/notebook.py:242\u001b[0m, in \u001b[0;36mtqdm_notebook.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/tqdm/notebook.py?line=239'>240</a>\u001b[0m unit_scale \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munit_scale \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munit_scale \u001b[39mor\u001b[39;00m \u001b[39m1\u001b[39m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/tqdm/notebook.py?line=240'>241</a>\u001b[0m total \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal \u001b[39m*\u001b[39m unit_scale \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal\n\u001b[0;32m--> <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/tqdm/notebook.py?line=241'>242</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstatus_printer(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp, total, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdesc, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mncols)\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/tqdm/notebook.py?line=242'>243</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontainer\u001b[39m.\u001b[39mpbar \u001b[39m=\u001b[39m proxy(\u001b[39mself\u001b[39m)\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/tqdm/notebook.py?line=243'>244</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdisplayed \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/tqdm/notebook.py:115\u001b[0m, in \u001b[0;36mtqdm_notebook.status_printer\u001b[0;34m(_, total, desc, ncols)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/tqdm/notebook.py?line=105'>106</a>\u001b[0m \u001b[39m# Fallback to text bar if there's no total\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/tqdm/notebook.py?line=106'>107</a>\u001b[0m \u001b[39m# DEPRECATED: replaced with an 'info' style bar\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/tqdm/notebook.py?line=107'>108</a>\u001b[0m \u001b[39m# if not total:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/tqdm/notebook.py?line=111'>112</a>\u001b[0m \n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/tqdm/notebook.py?line=112'>113</a>\u001b[0m \u001b[39m# Prepare IPython progress bar\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/tqdm/notebook.py?line=113'>114</a>\u001b[0m \u001b[39mif\u001b[39;00m IProgress \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# #187 #451 #558 #872\u001b[39;00m\n\u001b[0;32m--> <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/tqdm/notebook.py?line=114'>115</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/tqdm/notebook.py?line=115'>116</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIProgress not found. Please update jupyter and ipywidgets.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/tqdm/notebook.py?line=116'>117</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m See https://ipywidgets.readthedocs.io/en/stable\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/tqdm/notebook.py?line=117'>118</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m/user_install.html\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/tqdm/notebook.py?line=118'>119</a>\u001b[0m \u001b[39mif\u001b[39;00m total:\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/tqdm/notebook.py?line=119'>120</a>\u001b[0m     pbar \u001b[39m=\u001b[39m IProgress(\u001b[39mmin\u001b[39m\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, \u001b[39mmax\u001b[39m\u001b[39m=\u001b[39mtotal)\n",
      "\u001b[0;31mImportError\u001b[0m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    epoch_loss = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        tokens = tokens_to_cuda(batch['tokens'], device)\n",
    "        scores = model(tokens)    \n",
    "                \n",
    "        loss = loss_function(scores, batch['label'].to(device)) / batch_size\n",
    "\n",
    "        epoch_loss += loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "       \n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader):\n",
    "            tokens = tokens_to_cuda(batch['tokens'], device)\n",
    "            scores = model(tokens)    \n",
    "            loss = loss_function(scores, batch['label'].to(device)) / batch_size\n",
    "\n",
    "            val_loss += loss\n",
    "        \n",
    "    print(\"End of Epoch\", epoch)\n",
    "    print(\"Training loss:\", epoch_loss)\n",
    "    print(\"Test loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "97a03c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14b48f639afd4fe3aa3fae53a0a7d937",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3211 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2354247/2583769534.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.tokens.items()}\n",
      "/tmp/ipykernel_2354247/808527672.py:7: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  scores = F.softmax(model(tokens))\n",
      "/tmp/ipykernel_2354247/808527672.py:8: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  preds.extend(torch.round(F.softmax(scores)[:,1]).to(torch.int64))\n",
      "/tmp/ipykernel_2354247/808527672.py:9: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  right += torch.count_nonzero(batch['label'].to(device) == torch.round(F.softmax(scores)[:,1]).to(torch.int64))\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        tokens = tokens_to_cuda(batch['tokens'], device)\n",
    "        scores = F.softmax(model(tokens))\n",
    "        preds.extend(torch.round(F.softmax(scores)[:,1]).to(torch.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7ad4ce3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of training test accuracy tensor(0.9371, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "preds_cpu = [i.to('cpu') for i in preds]\n",
    "print(\"End of training f1 score accuracy\", f1_score(test_scores, preds_cpu))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7f0fb73b74445d6392909c9d469b21cbfa1ec308c4aeb674f6b2b586eed9f638"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
