{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass ReviewDataset(torch.utils.data.Dataset):\\n    def __init__(self, tokens, labels, ids):\\n        self.tokens = tokens\\n        self.labels = labels\\n        self.ids = ids\\n        \\n    def __len__(self):\\n        return len(self.tokens[\\'input_ids\\'])\\n\\n    def __getitem__(self, idx):\\n        item = {key: torch.tensor(val[idx]) for key, val in self.tokens.items()}\\n        out = {\"tokens\": item, \"label\": self.labels[idx], \"id\": self.ids[idx]}\\n        return out\\n\\ndef process_text(filepath, batch_size):\\n    reviews = []\\n    data = open(filepath)\\n    for line in data.readlines():\\n        reviews.append(json.loads(line))\\n\\n    review_texts = []\\n    review_scores = []\\n\\n    for sample in reviews:\\n        if \\'reviewText\\' in sample and \\'overall\\' in sample:\\n            review_texts.append(sample[\\'reviewText\\'])\\n            if sample[\\'overall\\'] >= 4:\\n                review_scores.append(1)\\n            else:\\n                review_scores.append(0)\\n                \\n    train_reviews = review_texts[:len(review_texts)//2]\\n    train_ids = [i for i in range(0, len(review_texts)//2+1)]\\n    test_reviews = review_texts[len(review_texts)//2:]\\n    test_ids = [i for i in range(len(review_texts)//2, len(review_texts))]\\n    train_scores = review_scores[:len(review_texts)//2]\\n    test_scores = review_scores[len(review_texts)//2:]\\n\\n    tokenizer = BertTokenizer.from_pretrained(\\'bert-base-uncased\\')\\n\\n    tokenized_train_reviews = tokenizer(train_reviews, return_tensors=\"pt\", padding=\\'max_length\\', truncation=True)\\n    tokenized_test_reviews = tokenizer(test_reviews, return_tensors=\"pt\", padding=\\'max_length\\', truncation=True)\\n\\n    train_dataset = ReviewDataset(tokenized_train_reviews, train_scores, train_ids)\\n    test_dataset = ReviewDataset(tokenized_test_reviews, test_scores, test_ids)\\n\\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\\n    return train_loader, test_loader\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cl_graph_bert as cgm\n",
    "import torch\n",
    "from torch import nn\n",
    "import json\n",
    "from transformers import BertModel, BertConfig, BertTokenizer, AdamW\n",
    "import tqdm\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "'''\n",
    "class ReviewDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokens, labels, ids):\n",
    "        self.tokens = tokens\n",
    "        self.labels = labels\n",
    "        self.ids = ids\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.tokens['input_ids'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.tokens.items()}\n",
    "        out = {\"tokens\": item, \"label\": self.labels[idx], \"id\": self.ids[idx]}\n",
    "        return out\n",
    "\n",
    "def process_text(filepath, batch_size):\n",
    "    reviews = []\n",
    "    data = open(filepath)\n",
    "    for line in data.readlines():\n",
    "        reviews.append(json.loads(line))\n",
    "\n",
    "    review_texts = []\n",
    "    review_scores = []\n",
    "\n",
    "    for sample in reviews:\n",
    "        if 'reviewText' in sample and 'overall' in sample:\n",
    "            review_texts.append(sample['reviewText'])\n",
    "            if sample['overall'] >= 4:\n",
    "                review_scores.append(1)\n",
    "            else:\n",
    "                review_scores.append(0)\n",
    "                \n",
    "    train_reviews = review_texts[:len(review_texts)//2]\n",
    "    train_ids = [i for i in range(0, len(review_texts)//2+1)]\n",
    "    test_reviews = review_texts[len(review_texts)//2:]\n",
    "    test_ids = [i for i in range(len(review_texts)//2, len(review_texts))]\n",
    "    train_scores = review_scores[:len(review_texts)//2]\n",
    "    test_scores = review_scores[len(review_texts)//2:]\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    tokenized_train_reviews = tokenizer(train_reviews, return_tensors=\"pt\", padding='max_length', truncation=True)\n",
    "    tokenized_test_reviews = tokenizer(test_reviews, return_tensors=\"pt\", padding='max_length', truncation=True)\n",
    "\n",
    "    train_dataset = ReviewDataset(tokenized_train_reviews, train_scores, train_ids)\n",
    "    test_dataset = ReviewDataset(tokenized_test_reviews, test_scores, test_ids)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "    return train_loader, test_loader\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "g = dgl.load_graphs(\"./graphs/industrial_and_scientific_5_core_new.dgl\")[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "num_reviews = g.num_nodes(\"Review\")\n",
    "'''\n",
    "class IDDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, start, end):\n",
    "        super(IDDataset).__init__()\n",
    "        assert end > start, \"this example code only works with end >= start\"\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "\n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        if worker_info is None:  # single-process data loading, return the full iterator\n",
    "            iter_start = self.start\n",
    "            iter_end = self.end\n",
    "        else:  # in a worker process\n",
    "            # split workload\n",
    "            per_worker = int(math.ceil((self.end - self.start) / float(worker_info.num_workers)))\n",
    "            worker_id = worker_info.id\n",
    "            iter_start = self.start + worker_id * per_worker\n",
    "            iter_end = min(iter_start + per_worker, self.end)\n",
    "        return iter(range(iter_start, iter_end))\n",
    "\n",
    "'''\n",
    "\n",
    "class IDDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, ids):\n",
    "        self.ids = ids\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out = {\"id\": self.ids[idx]}\n",
    "        return out\n",
    "\n",
    "train_ds = IDDataset(torch.tensor([i for i in range(0, num_reviews//2+1)]))\n",
    "test_ds = IDDataset(torch.tensor([i for i in range(num_reviews//2, num_reviews)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = cgm.CLIPGraphModel(\n",
    "    rel_types = g.etypes,\n",
    "    emb_types = {x: g.number_of_nodes(x) for x in g.ntypes} \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam([{\"params\":model.language_model.parameters(),\"lr\": 0.00001},\n",
    "                              {\"params\":model.graph_model.parameters(), \"lr\": 0.001},\n",
    "                              {\"params\":model.language_projection.parameters(), \"lr\": 0.001},\n",
    "                              {\"params\":model.graph_projection.parameters(), \"lr\": 0.001}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2409 [00:17<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/parkererickson/nlp8980/final_project/graph_bert_cl.ipynb Cell 7'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/parkererickson/nlp8980/final_project/graph_bert_cl.ipynb#ch0000006?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m tqdm\u001b[39m.\u001b[39mtqdm(train_loader):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/parkererickson/nlp8980/final_project/graph_bert_cl.ipynb#ch0000006?line=7'>8</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/parkererickson/nlp8980/final_project/graph_bert_cl.ipynb#ch0000006?line=8'>9</a>\u001b[0m     loss \u001b[39m=\u001b[39m model(g\u001b[39m.\u001b[39;49mto(device), \u001b[39m\"\u001b[39;49m\u001b[39mReview\u001b[39;49m\u001b[39m\"\u001b[39;49m, batch[\u001b[39m\"\u001b[39;49m\u001b[39mid\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mto(device))[\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m]    \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/parkererickson/nlp8980/final_project/graph_bert_cl.ipynb#ch0000006?line=10'>11</a>\u001b[0m     epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/parkererickson/nlp8980/final_project/graph_bert_cl.ipynb#ch0000006?line=12'>13</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/nlp8980/final_project/cl_graph_bert.py:98\u001b[0m, in \u001b[0;36mCLIPGraphModel.forward\u001b[0;34m(self, g, vertex_type, ids)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/parkererickson/nlp8980/final_project/cl_graph_bert.py?line=95'>96</a>\u001b[0m         graph_emb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgraph_projection(graph_output)\n\u001b[1;32m     <a href='file:///Users/parkererickson/nlp8980/final_project/cl_graph_bert.py?line=96'>97</a>\u001b[0m         \u001b[39m# Shape of emb_dim x batch_size\u001b[39;00m\n\u001b[0;32m---> <a href='file:///Users/parkererickson/nlp8980/final_project/cl_graph_bert.py?line=97'>98</a>\u001b[0m         language_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlanguage_model(input_ids \u001b[39m=\u001b[39;49m g\u001b[39m.\u001b[39;49mnodes[\u001b[39m\"\u001b[39;49m\u001b[39mReview\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mdata[\u001b[39m'\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m'\u001b[39;49m][ids], \n\u001b[1;32m     <a href='file:///Users/parkererickson/nlp8980/final_project/cl_graph_bert.py?line=98'>99</a>\u001b[0m            attention_mask\u001b[39m=\u001b[39;49mg\u001b[39m.\u001b[39;49mnodes[\u001b[39m\"\u001b[39;49m\u001b[39mReview\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mdata[\u001b[39m'\u001b[39;49m\u001b[39mattention_mask\u001b[39;49m\u001b[39m'\u001b[39;49m][ids], \n\u001b[1;32m    <a href='file:///Users/parkererickson/nlp8980/final_project/cl_graph_bert.py?line=99'>100</a>\u001b[0m            token_type_ids\u001b[39m=\u001b[39;49mg\u001b[39m.\u001b[39;49mnodes[\u001b[39m\"\u001b[39;49m\u001b[39mReview\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mdata[\u001b[39m'\u001b[39;49m\u001b[39mtoken_type_ids\u001b[39;49m\u001b[39m'\u001b[39;49m][ids])\u001b[39m.\u001b[39mlast_hidden_state[:,\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtype(torch\u001b[39m.\u001b[39mfloat64)\n\u001b[1;32m    <a href='file:///Users/parkererickson/nlp8980/final_project/cl_graph_bert.py?line=100'>101</a>\u001b[0m \u001b[39m#         print(language_output.shape)\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/parkererickson/nlp8980/final_project/cl_graph_bert.py?line=101'>102</a>\u001b[0m         language_emb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlanguage_projection(language_output)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:996\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=986'>987</a>\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=988'>989</a>\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=989'>990</a>\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=990'>991</a>\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=993'>994</a>\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=994'>995</a>\u001b[0m )\n\u001b[0;32m--> <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=995'>996</a>\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=996'>997</a>\u001b[0m     embedding_output,\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=997'>998</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=998'>999</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=999'>1000</a>\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1000'>1001</a>\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1001'>1002</a>\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1002'>1003</a>\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1003'>1004</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1004'>1005</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1005'>1006</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1006'>1007</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1007'>1008</a>\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1008'>1009</a>\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:585\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=575'>576</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=576'>577</a>\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=577'>578</a>\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=581'>582</a>\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=582'>583</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=583'>584</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=584'>585</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=585'>586</a>\u001b[0m         hidden_states,\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=586'>587</a>\u001b[0m         attention_mask,\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=587'>588</a>\u001b[0m         layer_head_mask,\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=588'>589</a>\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=589'>590</a>\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=590'>591</a>\u001b[0m         past_key_value,\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=591'>592</a>\u001b[0m         output_attentions,\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=592'>593</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=594'>595</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=595'>596</a>\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:472\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=459'>460</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=460'>461</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=461'>462</a>\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=468'>469</a>\u001b[0m ):\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=469'>470</a>\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=470'>471</a>\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=471'>472</a>\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=472'>473</a>\u001b[0m         hidden_states,\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=473'>474</a>\u001b[0m         attention_mask,\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=474'>475</a>\u001b[0m         head_mask,\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=475'>476</a>\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=476'>477</a>\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=477'>478</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=478'>479</a>\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=480'>481</a>\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:402\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=391'>392</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=392'>393</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=393'>394</a>\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=399'>400</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=400'>401</a>\u001b[0m ):\n\u001b[0;32m--> <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=401'>402</a>\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=402'>403</a>\u001b[0m         hidden_states,\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=403'>404</a>\u001b[0m         attention_mask,\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=404'>405</a>\u001b[0m         head_mask,\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=405'>406</a>\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=406'>407</a>\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=407'>408</a>\u001b[0m         past_key_value,\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=408'>409</a>\u001b[0m         output_attentions,\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=409'>410</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=410'>411</a>\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=411'>412</a>\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:268\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=257'>258</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=258'>259</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=259'>260</a>\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=265'>266</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=266'>267</a>\u001b[0m ):\n\u001b[0;32m--> <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=267'>268</a>\u001b[0m     mixed_query_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mquery(hidden_states)\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=269'>270</a>\u001b[0m     \u001b[39m# If this is instantiated as a cross-attention module, the keys\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=270'>271</a>\u001b[0m     \u001b[39m# and values come from an encoder; the attention mask needs to be\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=271'>272</a>\u001b[0m     \u001b[39m# such that the encoder's padding tokens are not attended to.\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=272'>273</a>\u001b[0m     is_cross_attention \u001b[39m=\u001b[39m encoder_hidden_states \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/linear.py?line=101'>102</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/linear.py?line=102'>103</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/functional.py:1848\u001b[0m, in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/functional.py?line=1845'>1846</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight, bias):\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/functional.py?line=1846'>1847</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[39minput\u001b[39m, weight, bias), \u001b[39minput\u001b[39m, weight, bias\u001b[39m=\u001b[39mbias)\n\u001b[0;32m-> <a href='file:///opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/functional.py?line=1847'>1848</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, weight, bias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for batch in tqdm.tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(g.to(device), \"Review\", batch[\"id\"].to(device))[\"loss\"]    \n",
    "                \n",
    "        epoch_loss += loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "       \n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm.tqdm(test_loader):\n",
    "            loss = model(g.to(device), \"Review\", batch[\"id\"].to(device))[\"loss\"]\n",
    "\n",
    "            val_loss += loss\n",
    "        \n",
    "    print(\"End of Epoch\", epoch)\n",
    "    print(\"Training loss:\", epoch_loss)\n",
    "    print(\"Test loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertGraphMLP(nn.Module):\n",
    "    def __init__(self, model, encoder=\"language_emb\", num_labels=2, finetune=False):\n",
    "        super(BertGraphMLP, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.BertGraph = model\n",
    "        self.encoder = encoder\n",
    "        self.classifier = nn.Linear(512, num_labels)\n",
    "        self.finetune = finetune\n",
    "\n",
    "    def forward(self, g, ids):\n",
    "        if self.finetune:\n",
    "            out = self.BertGraph(g, \"Review\", ids)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                out = self.BertGraph(g, \"Review\", ids)\n",
    "        if self.encoder == \"language_emb\":\n",
    "            embs = out[\"language_emb\"]\n",
    "        elif self.encoder == \"graph_emb\":\n",
    "            embs = out[\"graph_emb\"]\n",
    "        elif self.encoder == \"mean\":\n",
    "            embs = (out[\"langauge_emb\"] + out[\"graph_emb\"])/2\n",
    "        else:\n",
    "            raise(\"Not Implemented\")\n",
    "        logits = self.classifier(embs)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/2409 [00:00<?, ?it/s]/tmp/ipykernel_2556236/3120124057.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.tokens.items()}\n",
      "100%|███████████████████████████████████████| 2409/2409 [09:44<00:00,  4.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(135.6381, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|█████████▋                              | 585/2409 [02:22<07:23,  4.11it/s]"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def evaluate_review_sentiment(model, encoder, finetune=False):\n",
    "    eval_model = BertGraphMLP(model, encoder, finetune=finetune)\n",
    "\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        [\n",
    "            {\"params\":eval_model.classifier.parameters(), \"lr\": 0.001},       \n",
    "    ])\n",
    "\n",
    "    epochs = 10\n",
    "    device= 'cuda'\n",
    "    eval_model.to(device)\n",
    "    if not(finetune):\n",
    "        eval_model.BertGraph.eval()\n",
    "    preds = []\n",
    "\n",
    "    for epoch in tqdm.tqdm(range(epochs)):\n",
    "        epoch_loss = 0\n",
    "        for batch in tqdm.tqdm(train_loader):\n",
    "            optimizer.zero_grad()  \n",
    "            embs = eval_model(g.to(device), batch[\"id\"].to(device))  \n",
    "            loss = loss_function(embs, batch['label'].to(device)) / batch_size\n",
    "            epoch_loss += loss\n",
    "            loss.backward()\n",
    "            optimizer.step()    \n",
    "        print(epoch_loss/len(train_loader))\n",
    "\n",
    "    eval_model.eval()\n",
    "    preds = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm.tqdm(test_loader):\n",
    "            scores = eval_model(g.to(device), batch[\"id\"].to(device))\n",
    "            preds.extend(torch.round(F.softmax(scores)[:,1]).to(torch.int64))\n",
    "            labels.extend(g.nodes[\"Review\"].data[\"Positive\"][batch[\"id\"]].to(torch.int64))\n",
    "\n",
    "    from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "    preds_cpu = [i.to('cpu') for i in preds]\n",
    "    labels_cpu = [i.to('cpu') for i in labels]\n",
    "    print(\"End of training f1 score accuracy\", f1_score(labels_cpu, preds_cpu))\n",
    "    print(\"End of training accuracy\", accuracy_score(labels_cpu, preds_cpu))\n",
    "    print(\"End of training precision\", precision_score(labels_cpu, preds_cpu))\n",
    "    print(\"End of training recall\", recall_score(labels_cpu, preds_cpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_methods = [\"language_emb\", \"graph_emb\", \"mean\"]\n",
    "\n",
    "for method in enc_methods:\n",
    "    print(\"====== EVALUATING:\", method, \"======\")\n",
    "    evaluate_review_sentiment(model, method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./base_statedict_{}.pt\".format(float(val_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(68201)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.nodes[\"Review\"].data[\"Positive\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7f0fb73b74445d6392909c9d469b21cbfa1ec308c4aeb674f6b2b586eed9f638"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
