{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cl_graph_bert as cgm\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import dgl\n",
    "g = dgl.load_graphs(\"./graphs/industrial_and_scientific_5_core.dgl\")[0][0]\n",
    "\n",
    "model = cgm.CLIPGraphModel(\n",
    "    rel_types = g.etypes,\n",
    "    emb_types = {x: g.number_of_nodes(x) for x in g.ntypes} \n",
    ")\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "model.load_state_dict(torch.load(\"./base_statedict_6668.011407389138.pt\", map_location=torch.device(device)))\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Graph - Frozen GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, gnn_model, out_dim, freeze_base=True):\n",
    "        super().__init__()\n",
    "        self.mdl = gnn_model\n",
    "        self.freeze = freeze_base\n",
    "        self.linear = nn.Linear(model.graph_projection.projection_dim, out_dim)\n",
    "        self.act = nn.ReLU()\n",
    "        self.soft = nn.Softmax()\n",
    "\n",
    "    def forward(self, g):\n",
    "        if self.freeze:\n",
    "            with torch.no_grad():\n",
    "                x = self.mdl.graph_projection(self.mdl.graph_model(g)[\"Review\"].double()).float()\n",
    "        else:\n",
    "            x = self.mdl.graph_projection(self.mdl.graph_model(g)[\"Review\"].double()).float()\n",
    "        x = self.act(x)\n",
    "        x = self.linear(x)\n",
    "        out = self.soft(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m8/dqp9wj910q53mc6120n2z62m0000gn/T/ipykernel_4234/3056711328.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  out = self.soft(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 1.1762685775756836\n",
      "Epoch: 1 Loss: 1.1353669166564941\n",
      "Epoch: 2 Loss: 1.1285780668258667\n",
      "Epoch: 3 Loss: 0.420133501291275\n",
      "Epoch: 4 Loss: 0.42006874084472656\n",
      "Epoch: 5 Loss: 0.42010149359703064\n",
      "Epoch: 6 Loss: 0.4199940860271454\n",
      "Epoch: 7 Loss: 0.42008885741233826\n",
      "Epoch: 8 Loss: 0.42007431387901306\n",
      "Epoch: 9 Loss: 0.42003756761550903\n",
      "Epoch: 10 Loss: 0.4199889302253723\n",
      "Epoch: 11 Loss: 0.4200495481491089\n",
      "Epoch: 12 Loss: 0.4200839102268219\n",
      "Epoch: 13 Loss: 0.42009052634239197\n",
      "Epoch: 14 Loss: 0.42005789279937744\n",
      "Epoch: 15 Loss: 0.4200589954853058\n",
      "Epoch: 16 Loss: 0.4200672209262848\n",
      "Epoch: 17 Loss: 0.4199753999710083\n",
      "Epoch: 18 Loss: 0.42000052332878113\n",
      "Epoch: 19 Loss: 0.42002803087234497\n"
     ]
    }
   ],
   "source": [
    "gnn_cls = Classifier(model, 2, freeze_base=True)\n",
    "\n",
    "labels = g.nodes[\"Review\"].data[\"Positive\"]\n",
    "train_mask = g.nodes[\"Review\"].data[\"train_mask\"]\n",
    "test_mask = g.nodes[\"Review\"].data[\"test_mask\"]\n",
    "\n",
    "opt = torch.optim.Adam(gnn_cls.parameters())\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    logits = gnn_cls(g)\n",
    "    loss = F.cross_entropy(logits[train_mask == 1], labels[train_mask == 1].type(torch.long))\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    print(\"Epoch:\", epoch, \"Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m8/dqp9wj910q53mc6120n2z62m0000gn/T/ipykernel_4234/3056711328.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  out = self.soft(x)\n"
     ]
    }
   ],
   "source": [
    "gnn_cls.eval()\n",
    "preds = gnn_cls(g)[test_mask == 1].detach().numpy()\n",
    "y_test = labels[test_mask == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': 0.9343215065955035,\n",
       " 'recall': 1.0,\n",
       " 'precision': 0.8767386340045672,\n",
       " 'acc': 0.8767386340045672}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def metrics(y_test, preds):\n",
    "    mets = {}\n",
    "    mets[\"f1\"] = f1_score(y_test, preds.argmax(1))\n",
    "    mets[\"recall\"] = recall_score(y_test, preds.argmax(1))\n",
    "    mets[\"precision\"] = precision_score(y_test, preds.argmax(1))\n",
    "    mets[\"acc\"] = accuracy_score(y_test, preds.argmax(1))\n",
    "    return mets\n",
    "\n",
    "metrics(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Graph - Fine-tunable GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m8/dqp9wj910q53mc6120n2z62m0000gn/T/ipykernel_4234/3056711328.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  out = self.soft(x)\n",
      "/opt/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 0.4409789443016052\n",
      "Epoch: 1 Loss: 0.4202536344528198\n",
      "Epoch: 2 Loss: 0.42011967301368713\n",
      "Epoch: 3 Loss: 0.420083612203598\n",
      "Epoch: 4 Loss: 0.4200218617916107\n",
      "Epoch: 5 Loss: 0.42001932859420776\n",
      "Epoch: 6 Loss: 0.4200417399406433\n",
      "Epoch: 7 Loss: 0.4200417399406433\n",
      "Epoch: 8 Loss: 0.42007875442504883\n",
      "Epoch: 9 Loss: 0.4200665354728699\n",
      "Epoch: 10 Loss: 0.4199981093406677\n",
      "Epoch: 11 Loss: 0.4199976623058319\n",
      "Epoch: 12 Loss: 0.4200596511363983\n",
      "Epoch: 13 Loss: 0.4200640022754669\n",
      "Epoch: 14 Loss: 0.42006736993789673\n",
      "Epoch: 15 Loss: 0.42004090547561646\n",
      "Epoch: 16 Loss: 0.42000678181648254\n",
      "Epoch: 17 Loss: 0.42009252309799194\n",
      "Epoch: 18 Loss: 0.41996869444847107\n",
      "Epoch: 19 Loss: 0.4200308322906494\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'f1': 0.9343215065955035,\n",
       " 'recall': 1.0,\n",
       " 'precision': 0.8767386340045672,\n",
       " 'acc': 0.8767386340045672}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnn_cls_unfrozen = Classifier(model, 2, freeze_base=False)\n",
    "\n",
    "labels = g.nodes[\"Review\"].data[\"Positive\"]\n",
    "train_mask = g.nodes[\"Review\"].data[\"train_mask\"]\n",
    "test_mask = g.nodes[\"Review\"].data[\"test_mask\"]\n",
    "\n",
    "opt =  torch.optim.Adam(\n",
    "        [\n",
    "            {\"params\":gnn_cls_unfrozen.mdl.parameters(), \"lr\": 0.00001}, \n",
    "            {\"params\":gnn_cls_unfrozen.linear.parameters(), \"lr\": 0.001}      \n",
    "    ])\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    logits = gnn_cls_unfrozen(g)\n",
    "    loss = F.cross_entropy(logits[train_mask == 1], labels[train_mask == 1].type(torch.long))\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    print(\"Epoch:\", epoch, \"Loss:\", loss.item())\n",
    "\n",
    "gnn_cls_unfrozen.eval()\n",
    "preds = gnn_cls_unfrozen(g)[test_mask == 1].detach().numpy()\n",
    "y_test = labels[test_mask == 1]\n",
    "\n",
    "metrics(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_out = model.graph_projection(model.graph_model(g)[\"Review\"].double()).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3,  4,  5,  7,  8, 11, 12, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 28,\n",
       "        29, 30, 32, 33])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prod_id = 1\n",
    "review_id = g.successors(prod_id, 'rev_REVIEW_OF')\n",
    "review_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.7271e+02,  1.3971e+01,  4.9848e-02,  ...,  2.9182e+02,\n",
       "          3.4543e+03,  2.4162e+02],\n",
       "        [ 2.5599e+02,  1.3208e+01, -2.7077e+00,  ...,  2.7823e+02,\n",
       "          3.3170e+03,  2.2543e+02],\n",
       "        [ 2.7113e+02,  1.8487e+01,  1.0898e+01,  ...,  2.8357e+02,\n",
       "          3.2502e+03,  2.4358e+02],\n",
       "        ...,\n",
       "        [ 3.2835e+02,  3.2599e+01,  3.3280e+01,  ...,  3.3396e+02,\n",
       "          3.6118e+03,  3.0059e+02],\n",
       "        [ 2.7287e+02, -2.1590e-01, -3.2043e+01,  ...,  3.1011e+02,\n",
       "          3.9872e+03,  2.3210e+02],\n",
       "        [ 2.8199e+02,  2.8142e+01,  2.6989e+01,  ...,  2.8999e+02,\n",
       "          3.1588e+03,  2.5726e+02]], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_out[review_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model.language_model(input_ids = tokens['input_ids'], \n",
    "           attention_mask=tokens['attention_mask'], \n",
    "           token_type_ids=tokens['token_type_ids']).last_hidden_state[:,0].type(torch.float64)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7f0fb73b74445d6392909c9d469b21cbfa1ec308c4aeb674f6b2b586eed9f638"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('huggingface')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
